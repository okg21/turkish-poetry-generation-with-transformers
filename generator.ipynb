{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8jX_pDdtoB7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHPQKuS8tiDu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/distilbert-base-turkish-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am4ycrZlMlXw"
      },
      "outputs": [],
      "source": [
        "tokenizer.src_lang = 'tr'\n",
        "tokenizer.add_tokens('<br/>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HyvifRgLXCW"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.tokenize(\"merhaba benim adım kaan efe keleştir geldim gittim <br/> çekoslavaykyalılaştırdım\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFNGDpKJyQcJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "import pandas as pd\n",
        "import torch.distributions as dist\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1tDF0wvjkvg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "#mask function for the attention\n",
        "def apply_mask(matrices, maskval, mask_diagonal=True):\n",
        "    h, w = matrices.size(-2), matrices.size(-1)\n",
        "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
        "    matrices[..., indices[0], indices[1]] = maskval\n",
        "\n",
        "\n",
        "class multi_head_attention(nn.Module):\n",
        "  def __init__(self, d_model, mask, heads=8):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.heads = heads\n",
        "    self.mask = mask\n",
        "\n",
        "    self.keyWeights = nn.Linear(d_model, d_model*heads, bias=False)\n",
        "    self.queryWeights = nn.Linear(d_model, d_model*heads, bias=False)\n",
        "    self.valueWeights = nn.Linear(d_model, d_model*heads, bias=False)\n",
        "\n",
        "    self.concatHeads = nn.Linear(d_model*heads, d_model)\n",
        "\n",
        "  def forward(self, v, k, q):\n",
        "    #b:batch_len, s:sequence_len, e:encoding_len\n",
        "    b,s,e = v.size()\n",
        "    h = self.heads\n",
        "\n",
        "    #e must be divisible by h otherwise throw error\n",
        "    if e % h != 0:\n",
        "      print(\"Head size should be a diviser of embedding length\")\n",
        "      return\n",
        "\n",
        "    #apply the weights on inputs and then reshape to b,s,h,e to be able to move head dimension later\n",
        "    keys  = self.keyWeights(k).view(b,s,h,e)\n",
        "    queries  = self.queryWeights(q).view(b,s,h,e)\n",
        "    values  = self.valueWeights(v).view(b,s,h,e)\n",
        "\n",
        "    #we need to move batch and head dimension next to each other to form a b*h sized dimension\n",
        "    keys = keys.transpose(1, 2).reshape(b * h, s, e)\n",
        "    queries = queries.transpose(1, 2).reshape(b * h, s, e)\n",
        "    values = values.transpose(1, 2).reshape(b * h, s, e)\n",
        "\n",
        "    #queries b*h,s,e X keys: b*h,e,s so output dimensions: b*h,s,s\n",
        "    dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "    scaled_dot = dot / (e**(1/2))\n",
        "    \n",
        "    #add the mask to the scaled dot product\n",
        "    if self.mask:\n",
        "      apply_mask(dot, float('-inf'), mask_diagonal=False)\n",
        "    #apply softmax\n",
        "    attention_weights = nn.functional.softmax(dot, dim=2)\n",
        "\n",
        "    #apply self-attention to values mul's result dimension: b*h,s,e\n",
        "    #we will reshape to b,h,s,e\n",
        "    out = torch.bmm(attention_weights, values).view(b, h, s, e)\n",
        "\n",
        "    #swap h and s dimension to concat all the h's over e dimensions\n",
        "    out = out.transpose(1,2).reshape(b, s, h*e)\n",
        "    return self.concatHeads(out)\n",
        "\n",
        "#TO-DO: check weight initilizations for all parameter types\n",
        "class position_wise_feed_forward(nn.Module):\n",
        "  def __init__(self, d_model, dff):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=dff, kernel_size=1)\n",
        "    self.conv2 = nn.Conv1d(in_channels=dff, out_channels=d_model, kernel_size=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #input is batch_size, seq_len, d_model\n",
        "    #since conv expects the channel dimension on second dimension, transpose\n",
        "    conv1_out = nn.functional.relu(self.conv1(x.transpose(1, 2)))\n",
        "    #second conv's output will be batch_size, d_model, seq_len, transpose to acquire input dimensions\n",
        "    conv2_out = self.conv2(conv1_out).transpose(1,2)\n",
        "    return conv2_out\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, mask, heads, dff, dp_rate):\n",
        "    super().__init__()\n",
        "    self.multi_head_attention = multi_head_attention(d_model, mask, heads)\n",
        "    self.add_norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "    self.dropout1 = nn.Dropout(dp_rate)\n",
        "\n",
        "    self.ffn = position_wise_feed_forward(d_model, dff)\n",
        "    self.add_norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "    self.dropout2 = nn.Dropout(dp_rate)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    attention = self.multi_head_attention(x,x,x)\n",
        "    attention = self.dropout1(attention)\n",
        "    res_connection = x + attention\n",
        "    add_normalized = self.add_norm1(res_connection)\n",
        "\n",
        "    ff_out = self.ffn(add_normalized)\n",
        "    ff_out = self.dropout2(ff_out)\n",
        "    res_connection2 = add_normalized + ff_out\n",
        "    add_normalized2 = self.add_norm2(res_connection2)\n",
        "    \n",
        "    return add_normalized2\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "  return torch.from_numpy(pos_encoding)\n",
        "\n",
        "\n",
        "class EncoderStack(nn.Module):\n",
        "  def __init__(self, num_layers, mask, d_model, heads, dff, dp_rate, vocab_size, seq_length):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    #embeddings\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_embedding = nn.Embedding(embedding_dim=d_model, num_embeddings=seq_length)    \n",
        "    self.dropout = nn.Dropout(dp_rate)\n",
        "\n",
        "    #encoder layers\n",
        "    enc_layers = []\n",
        "    for i in range(num_layers):\n",
        "      enc_layers.append(EncoderLayer(d_model, mask, heads, dff, dp_rate))\n",
        "\n",
        "    self.enc_layers = nn.Sequential(*enc_layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #learned embeddings       \n",
        "    x = self.embedding(x)\n",
        "    x *= math.sqrt(self.d_model)\n",
        "    batch_size, seq_size, em_size = x.shape\n",
        "\n",
        "\n",
        "    #addition with positional encodings\n",
        "    x += self.pos_embedding(torch.arange(seq_size, device=device))[None, :, :].expand(batch_size, seq_size, em_size)\n",
        "    #dropout layer\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    #encoder stack\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "    return x\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class GeneratorTransformer(nn.Module):\n",
        "  def __init__(self, num_layers, d_model, heads, dff, dp_rate, vocab_size, seq_length, mask=True):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.encoder_stack  = EncoderStack(num_layers, mask, d_model, heads, dff, dp_rate, vocab_size, seq_length).to(device)\n",
        "    self.outprobs = nn.Linear(d_model, vocab_size)\n",
        "    self.init_weights()\n",
        "      \n",
        "  def init_weights(self):\n",
        "      self.outprobs.weight = self.encoder_stack.embedding.weight\n",
        "      self.apply(self._init_weights)\n",
        "  \n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, (nn.Linear, nn.Embedding, nn.Conv1d)):\n",
        "          module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "          if isinstance(module, (nn.Linear, nn.Conv1d)) and module.bias is not None:\n",
        "              module.bias.data.zero_()\n",
        "      elif isinstance(module, nn.LayerNorm):\n",
        "          module.bias.data.zero_()\n",
        "          module.weight.data.fill_(1.0)\n",
        "        \n",
        "  def forward(self, x):\n",
        "    batch_size, sequence_size = x.size()\n",
        "    \n",
        "    x = self.encoder_stack(x)\n",
        "    x = self.outprobs(x.view(batch_size * sequence_size, self.d_model)).view(batch_size, sequence_size, self.vocab_size)\n",
        "    softmax = nn.functional.log_softmax(x, dim=2)\n",
        "    return softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKWeEE-91dmn"
      },
      "outputs": [],
      "source": [
        "poems1 = pd.read_csv('poems.csv')\n",
        "poems2_f = open(\"poems2.txt\")\n",
        "poems2 = poems2_f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b672SrdmoJEY"
      },
      "outputs": [],
      "source": [
        "poems1['poem']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTrhu-AYWLj9"
      },
      "outputs": [],
      "source": [
        "chars_to_remove = ['\\t', ':', 'î'\n",
        "       'û', '(', ')', '—', '…',\n",
        "       'ə',  '”', '*', '“', '\"', 'j', 'w', '«', '»',\n",
        "        'Â', '_',  '‘',  'à',  '`',\n",
        "       'W', '–', 'ê', '=',  '´', '[', 'Î', 'Ə', 'ä', 'ß',\n",
        "       '•', 'é', ']', '@', 'ô', '+', 'ù', '&', '¥', '\\xa0', 'ý', '·', 'Û',\n",
        "       '|', 'Ò', '‚', '%', '^', '¹', '\\u200e', 'Í', 'Ã', '#', 'ú', 'è',\n",
        "       '„','</p>','<p>','...','0','1','2','3','4','5','6','7','8','9',\"quot;\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sUPvXwRoPa2"
      },
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "for poem in poems1['poem']:\n",
        "    poemer = poem\n",
        "    for i in chars_to_remove:\n",
        "        poemer = poemer.replace(i,' ')\n",
        "    text = text + poemer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in chars_to_remove:\n",
        "    poems2 = poems2.replace(i,' ')\n",
        "text = text + \" \" + poems2"
      ],
      "metadata": {
        "id": "Rn7ey1oiZe6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrUREHZjW-1-"
      },
      "outputs": [],
      "source": [
        "text = re.sub('<br>','<br/>',text)\n",
        "text = re.sub('<br/>',' <br/> ',text)\n",
        "text = re.sub('\\n','<br/>',text)\n",
        "text = re.sub('  *',' ',text)\n",
        "text = re.sub('\\.\\.*','.',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL8sVQO9vV7O"
      },
      "outputs": [],
      "source": [
        "tokenized_text = tokenizer.tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EiFKSU1vadl"
      },
      "outputs": [],
      "source": [
        "tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4RjK7tF7H9D"
      },
      "outputs": [],
      "source": [
        "uniques, counts = np.unique(tokenized_text, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbrejknhCFsy"
      },
      "outputs": [],
      "source": [
        "freq_sorted = np.flip(uniques[counts.argsort()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3iC_qfAS-kj"
      },
      "outputs": [],
      "source": [
        "freq_sorted[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3pL2Y4qMI7G"
      },
      "outputs": [],
      "source": [
        "vocab_dict = {}\n",
        "for i in range(len(freq_sorted)):\n",
        "    vocab_dict[freq_sorted[i]] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEiK35oHVGWM"
      },
      "outputs": [],
      "source": [
        "detok_vocab_dict = {}\n",
        "for i in range(len(freq_sorted)):\n",
        "    detok_vocab_dict[i] = freq_sorted[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6YJgZuNEMjl"
      },
      "outputs": [],
      "source": [
        "def tokenize(char_array, vocab):\n",
        "    tokenized = []\n",
        "    for i in range(len(char_array)):\n",
        "        if i%100000 == 0:\n",
        "            print(f\"%{i/len(char_array)*100}\",end='\\r')\n",
        "        tokenized.append(vocab.get(char_array[i]))\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lslsu41gU5Z9"
      },
      "outputs": [],
      "source": [
        "def de_tokenize(tok_array, vocab):\n",
        "    de_tokenize = []\n",
        "    for i in range(len(tok_array)):\n",
        "        if i%100000 == 0:\n",
        "            print(f\"%{i/len(tok_array)*100}\",end='\\r')\n",
        "        de_tokenize.append(vocab.get(tok_array[i]))\n",
        "    return de_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpF6HMAaEvSJ"
      },
      "outputs": [],
      "source": [
        "tokenized = tokenize(tokenized_text,vocab_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp8rNBX7o3xo"
      },
      "outputs": [],
      "source": [
        "tokenized_len = len(tokenized)\n",
        "train_val_split = 90\n",
        "train_data = torch.tensor(tokenized[:tokenized_len*train_val_split//100])\n",
        "validation_data = torch.tensor(tokenized[tokenized_len*train_val_split//100:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y-cr7CmPl31"
      },
      "outputs": [],
      "source": [
        "def split_to_batches(src, batch_size):\n",
        "    sequence_length = src.size(0) // batch_size\n",
        "    src = src[:sequence_length * batch_size]\n",
        "    src = src.view(batch_size, sequence_length).t().contiguous()\n",
        "    return src.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC_uuCI2Rk9S"
      },
      "outputs": [],
      "source": [
        "def get_batch_data(src, bptt, batch_count):\n",
        "    seq_len = min(bptt, len(src) - 1 - batch_count)\n",
        "    data = src[batch_count:batch_count+seq_len]\n",
        "    target = src[batch_count+1:batch_count+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-zixsk3tbwh"
      },
      "outputs": [],
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4qNpR5tuWDF"
      },
      "outputs": [],
      "source": [
        "def sampler(prompt ,out,temp = 0.6):\n",
        "    probabilities = F.softmax(out[0][-1] / temp, dim=0)\n",
        "    catout = dist.Categorical(probabilities)\n",
        "    prompt.append(de_tokenize([catout.sample().cpu().item()],detok_vocab_dict)[0])\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKppU9CtJYiV"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "batch_size = 4\n",
        "backpropagation_through_time = 1024\n",
        "device = \"cuda\"\n",
        "ntokens = len(vocab_dict)  # size of vocabulary\n",
        "emsize = 512  # embedding dimension\n",
        "d_hid = 512   # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 1 # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 4  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.1 # dropout probability\n",
        "model = GeneratorTransformer(num_layers=nlayers, d_model=emsize, heads=nhead, dff=d_hid, dp_rate=dropout, vocab_size=ntokens, seq_length=backpropagation_through_time, mask=True).to(device)\n",
        "\n",
        "trn_batch_split_tokens = split_to_batches(train_data,batch_size).to(device)\n",
        "val_batch_split_tokens = split_to_batches(validation_data,batch_size).to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "lr = 3e-4  # learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "train_hist = []\n",
        "eval_hist = []\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    start_time = time.time()\n",
        "    num_batches = len(trn_batch_split_tokens) // backpropagation_through_time\n",
        "    mean_losses = []\n",
        "    for batch, i in enumerate(range(0, trn_batch_split_tokens.size(0) - 1, backpropagation_through_time)):\n",
        "        data, targets = get_batch_data(trn_batch_split_tokens, backpropagation_through_time,i)\n",
        "        batch_size = data.size(0)\n",
        "        output = model(data)\n",
        "        #print(output.shape)\n",
        "        #print(targets.shape)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        mean_loss = torch.mean(loss).item()\n",
        "        mean_losses.append(mean_loss)\n",
        "        if batch % 100 == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000\n",
        "            cur_loss = mean_loss\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {get_lr(optimizer)} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'overall loss {np.mean(mean_losses)}')\n",
        "            start_time = time.time()\n",
        "    train_hist.append(np.mean(mean_losses))\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, backpropagation_through_time):\n",
        "            data, targets = get_batch_data(eval_data,backpropagation_through_time, i)\n",
        "            batch_size = data.size(0)\n",
        "            output = model(data)\n",
        "            total_loss.append(criterion(output.view(-1, ntokens), targets).item())\n",
        "    eval_hist.append(np.mean(total_loss))\n",
        "    return np.mean(total_loss)"
      ],
      "metadata": {
        "id": "9-8olRPRwQ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69fTi4ScqA3G"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 100\n",
        "best_model = None\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_batch_split_tokens)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    prompt = \"Yalnız\"\n",
        "    overall = prompt\n",
        "    prompt = tokenizer.tokenize(prompt)\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i in range(100):\n",
        "            prompt = prompt[-backpropagation_through_time:]\n",
        "            out = model(torch.tensor(tokenize(list(prompt),vocab_dict)).reshape(1,len(prompt)).to(device))\n",
        "            prompt = sampler(prompt, out,0.7)\n",
        "            if prompt[-1][0] == '▁':\n",
        "                 overall = overall + \" \" + prompt[-1][1:]\n",
        "            elif prompt[-1] == '<br/>':\n",
        "                 overall = overall + \"\\n\"           \n",
        "            else:\n",
        "                overall = overall + prompt[-1]\n",
        "        print(overall)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9o2qv_tJcnM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0ZUHgVfYmmc"
      },
      "outputs": [],
      "source": [
        "plt.plot(eval_hist)\n",
        "plt.plot(train_hist)\n",
        "plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wy6L3CqEV8t"
      },
      "outputs": [],
      "source": [
        "model = best_model\n",
        "prompt = \"Yağmuru seviyorum diyorsun,\\nyağmur yağınca şemsiyeni açıyorsun \\nGüneşi seviyorum diyorsun,\\ngüneş açınca gölgeye kaçıyorsun \\nRüzgarı seviyorum diyorsun,\\nrüzgar çıkınca pencereni kapatıyorsun\\nİşte,bunun için korkuyorum;\\nBeni de sevdiğini söylüyorsun..\"\n",
        "overall = prompt\n",
        "prompt = tokenizer.tokenize(prompt)\n",
        "with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i in range(300):\n",
        "            prompt = prompt[-backpropagation_through_time:]\n",
        "            out = model(torch.tensor(tokenize(list(prompt),vocab_dict)).reshape(1,len(prompt)).to(device))\n",
        "            prompt = sampler(prompt, out,1)\n",
        "            if prompt[-1][0] == '▁':\n",
        "                 overall = overall + \" \" + prompt[-1][1:]\n",
        "            elif prompt[-1] == '<br/>':\n",
        "                 overall = overall + \"\\n\"           \n",
        "            else:\n",
        "                overall = overall + prompt[-1]\n",
        "        print(overall)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " val_loss = evaluate(model, val_batch_split_tokens)\n",
        " print(val_loss)"
      ],
      "metadata": {
        "id": "BZ9w7nvNac77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc5aOGzKZfOH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfyaoFYqaGSb"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/transformer_models/' + str(batch_size) + '_' + str(backpropagation_through_time) + '_' + str(emsize) + '_' + str(d_hid) + '_' + str(nlayers) + '_' + str(nhead) + '.pth'\n",
        "torch.save(model, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aCBLyQxcE02"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/transformer_models/4_1024_512_512_1_4.pth\"\n",
        "model_loaded = torch.load(path)\n",
        "model_loaded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " eval_hist = []\n",
        " val_loss = evaluate(model_loaded, val_batch_split_tokens)\n",
        " print(val_loss)"
      ],
      "metadata": {
        "id": "sT-n0odTZ2Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Yalnızlık\"\n",
        "overall = prompt\n",
        "prompt = tokenizer.tokenize(prompt)\n",
        "with torch.no_grad():\n",
        "        for i in range(300):\n",
        "            prompt = prompt[-backpropagation_through_time:]\n",
        "            out = model_loaded(torch.tensor(tokenize(list(prompt),vocab_dict)).reshape(1,len(prompt)).to(device))\n",
        "            prompt = sampler(prompt, out,0.85)\n",
        "            if prompt[-1][0] == '▁':\n",
        "                 overall = overall + \" \" + prompt[-1][1:]\n",
        "            elif prompt[-1] == '<br/>':\n",
        "                 overall = overall + \"\\n\"           \n",
        "            else:\n",
        "                overall = overall + prompt[-1]\n",
        "        print(overall)"
      ],
      "metadata": {
        "id": "_h0ogxZlXsfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ceZG4ntew8t8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "generator.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}